{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Partie 1: Importation des Bibliothèques et Configuration Initiale**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "from google.colab import drive\n",
    "import shutil\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Explication:**\n",
    "- **google.colab.drive** : Permet de monter Google Drive pour sauvegarder les résultats.\n",
    "- **shutil** : Pour copier des fichiers et dossiers.\n",
    "- **torch** : Utilisé pour les opérations sur le GPU avec PyTorch.\n",
    "- **pandas** : Manipule les données sous forme de DataFrame.\n",
    "- **re** : Pour les opérations de traitement de texte.\n",
    "- **sklearn.model_selection.train_test_split** : Pour diviser les données en ensembles d'entraînement et de validation.\n",
    "- **transformers** : Fournit des outils pour le modèle T5 et la gestion de l'entraînement.\n",
    "- **datasets** : Permet de charger des datasets à partir de fichiers CSV.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Partie 2: Chargement et Préparation des Données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monter Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Charger les données à partir du fichier JSON\n",
    "df2 = pd.read_json(\"hf://datasets/ltg/en-wiki-paraphrased/train.jsonl\", lines=True)\n",
    "\n",
    "# Sauvegarder les données en CSV\n",
    "csv_file = 'en_wiki_paraphrased.csv'\n",
    "df2.to_csv(csv_file, index=False)\n",
    "\n",
    "# Charger les données CSV\n",
    "df1 = pd.read_csv(csv_file)\n",
    "df = df1.head(100000)\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de validation\n",
    "df_train, df_val = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Sauvegarder les ensembles en fichiers CSV\n",
    "output_file1 = 'train.csv'\n",
    "output_file2 = 'val.csv'\n",
    "df_train.to_csv(output_file1, index=False)\n",
    "df_val.to_csv(output_file2, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Explication:**\n",
    "- Les données sont chargées à partir d'un fichier JSON en utilisant `pandas`.\n",
    "- Les données sont ensuite sauvegardées au format CSV.\n",
    "- Le CSV est divisé en ensembles d'entraînement et de validation en utilisant `train_test_split`.\n",
    "- Les ensembles divisés sont sauvegardés en fichiers CSV distincts.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Partie 3: Chargement des Datasets et Prétraitement**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les datasets à partir des fichiers CSV\n",
    "dataset_train = load_dataset('csv', data_files='train.csv', split='train')\n",
    "dataset_valid = load_dataset('csv', data_files='val.csv', split='train')\n",
    "\n",
    "# Initialiser le tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\" paraphrase : {original}\" for (original) in zip(examples['original'])]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "    cleaned_tag = examples['paraphrase']\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            cleaned_tag,\n",
    "            max_length=MAX_LENGTH,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Appliquer la fonction de prétraitement aux datasets\n",
    "tokenized_train = dataset_train.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=NUM_PROCS  # Utiliser un seul processus\n",
    ")\n",
    "tokenized_valid = dataset_valid.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=NUM_PROCS  # Utiliser un seul processus\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Explication:**\n",
    "- Les datasets CSV sont chargés en utilisant `load_dataset`.\n",
    "- Le tokenizer T5 est initialisé pour transformer le texte.\n",
    "- La fonction `preprocess_function` est définie pour ajouter les balises nécessaires et tronquer/padd les textes.\n",
    "- La fonction de prétraitement est appliquée aux datasets d'entraînement et de validation.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Partie 4: Initialisation du Modèle et Arguments d'Entraînement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser le modèle T5\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Définir les arguments d'entraînement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=OUT_DIR,\n",
    "    logging_steps=10,\n",
    "    eval_strategy='steps',  # Déprécié, à remplacer par `eval_steps` dans la version future\n",
    "    save_steps=2000,\n",
    "    eval_steps=1000,  # Évalue la perte de validation tous les 100 pas\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=5,\n",
    "    report_to='tensorboard',\n",
    "    learning_rate=0.0001,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=0  # Réduire le nombre de travailleurs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Explication:**\n",
    "- Le modèle T5 est initialisé et déplacé sur le GPU si disponible.\n",
    "- Les arguments pour l'entraînement sont définis, y compris le nombre d'époques, la taille des lots, et la stratégie de sauvegarde.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **Partie 5: Entraînement du Modèle**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner le modèle\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,  # Dataset de validation (test)\n",
    ")\n",
    "\n",
    "history = trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Explication:**\n",
    "- Le `Trainer` de la bibliothèque `transformers` est utilisé pour entraîner le modèle sur les données préparées.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **Partie 6: Sauvegarde du Tokenizer et du Modèle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le tokenizer\n",
    "tokenizer.save_pretrained(OUT_DIR)\n",
    "\n",
    "# Copier les résultats vers Google Drive\n",
    "source_folder = f'./{OUT_DIR}'\n",
    "target_folder = f'/content/drive/My Drive/T5vfin/{OUT_DIR}'\n",
    "shutil.copytree(source_folder , target_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Explication:**\n",
    "- Le tokenizer est sauvegardé dans le répertoire spécifié.\n",
    "- Les résultats sont copiés dans Google Drive pour la sauvegarde.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Partie 7: Chargement du Modèle Sauvegardé et Fonction de Paraphrasage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Charger le modèle et le tokenizer sauvegardés\n",
    "model_path = './T5vfin/res_T5small/checkpoint-15000'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "tokenizer = T5Tokenizer.from_pretrained('./T5vfin/res_T5small')\n",
    "\n",
    "def paraphraser(text, model, tokenizer):\n",
    "    input_text = f\" paraphrase : {text}\"\n",
    "    inputs = tokenizer.encode(\n",
    "        input_text,\n",
    "        return_tensors='pt',\n",
    "        max_length=256,\n",
    "        padding='max_length',\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # Générer la paraphrase\n",
    "    corrected_ids = model.generate(\n",
    "        inputs,\n",
    "        max_length=256,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Décoder la paraphrase\n",
    "    paraphrase = tokenizer.decode(\n",
    "        corrected_ids[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    return paraphrase\n",
    "\n",
    "# Exemple de texte à paraphraser\n",
    "text = \"\"\"\n",
    "Des averses orageuses localement fortes et de fortes rafales de vent avec chasse-poussières locales sont prévues dimanche dans plusieurs provinces du Royaume, a annoncé la Direction générale de la météorologie (DGM). ...\n",
    "\"\"\"\n",
    "\n",
    "print(paraphraser(text, model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Explication:**\n",
    "- Le modèle et le tokenizer sauvegardés sont chargés.\n",
    "- La fonction `paraphraser` est définie pour générer des paraphrases du texte fourni.\n",
    "- Un exemple de texte est paraphrasé en utilisant le modèle.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
